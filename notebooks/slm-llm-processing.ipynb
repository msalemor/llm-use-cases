{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLM and LLM token count processing\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import common\n",
    "import tiktoken\n",
    "\n",
    "client = common.get_openai_client(api_key=common.api_KEY,\n",
    "        api_version=common.api_version,\n",
    "        azure_endpoint=common.api_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Ollama and Azure GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_completion(prompt:str, model='gpt4o'):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "        {\n",
    "           \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "    \n",
    "def ollama_completion(prompt:str, model='llava'):\n",
    "    response = ollama.chat(model=model, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    },\n",
    "    ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose to summarize based on token count with Ollama Llava or Azure OpenAI GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "def get_tokens(text: str) -> str:\n",
    "    return encoding.encode(text)\n",
    "\n",
    "def summarize(text: str) -> str:\n",
    "    template = f\"Summarize the following text in one paragraph:\\n{text}\"\n",
    "    token_count = len(get_tokens(template))\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    if  token_count> 4096:\n",
    "        # GPT processing\n",
    "        print(\"Processing with GPT\")\n",
    "        return gpt_completion(template)\n",
    "    else:\n",
    "        # Ollama processing\n",
    "        print(\"Processing with Ollama\")\n",
    "        return ollama_completion(template)\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a file and determine if it can be processed by Ollama locally or with Azure GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_data = read_file(\"data/401k.txt\")\n",
    "print(summarize(short_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a file and determine if it can be processed by Ollama locally or with Azure GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_data = read_file(\"data/aks_core_concepts.txt\")\n",
    "print(summarize(long_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
