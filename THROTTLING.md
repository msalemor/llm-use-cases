## Service Throttling<br/>Retry Logic

Many service providers including OpenAI usually set limits on the number of calls that can be made. In the case of Azure OpenAI, there are token limits (TPM or tokens per minute) and limits on the number of requests per minute. When a server starts running out of resources or the service limits are exhausted, the provider may issue a 429 or TooManyRequests HTTP Status code, and also a Retry-After response header indicating how much time you should wait until you try the next request.

As part of development best practices, applications should include retry logic for 429s and other retriable HTTP statuses as well as a good user experience when a service is not available or busy. If a backend service is not infinitely scalable and the customer is willing to pay for additional capacity, I've read posts about making calls to two or more load-balanced endpoints; possibly on a service such as APIM. I think that this may be part of a comprehensive solution. I read of other solutions that load balance and wait on an API management server. I humbly disagree that waiting on the server is a good solution.

There are many resilience and fault-handling libraries like Polly. However, I wrote the following "plain" .NET 8 code as a custom HttpClient with a round-robin retry handler that can perform load-balancing between two endpoints and takes into consideration the Retry-After header if available when retrying. I also wrote a backend simulator to test the policies and behavior. The retry handler constructor takes a list of endpoints, the number of retries, and the default delay in case the service does not provide a retry-after header. The code can be found here: https://lnkd.in/ebn6aAPZ

In the case of Azure OpenAI, other recommendations include looking at the model being used and setting the right amount of max tokens in the prompt. For example, GPT 3.5 offers a higher TPM than GPT 4.0. Now this is changing, and the plans and prices keep changing. If you want to get more requests out of your plan, you may ask yourself if using GPT 3.5 is appropriate to meet the application's requirements.

All these issues need to be considered including good UX/workflow design, adding application resiliency and fault-handling logic, the backend, service limits, choosing the right model for the job, the API policies, logging and monitoring, networking considerations, etc. when working through these issues.
