# Vector databases and embeddings in the development of AI applications

An embedding is a vector; a numerical representation of a piece of information, for example, text. A vector database is a place where you can store the embedding along with the piece of information. In many applications that implement RAG pattern for example, to find information, the user's query will itself be turned into an embedding (using the same model as the embedding in the database), and it will be used to search the vector database comparing the two vectors using something called cosine similarity. The results will be ranked according to similarity.

There are many databases already that implement vector search, and there are others where cosine similarity can be implemented in code. For example, Semantic Kernel supports everything from volatile memory storage (RAM) to databases like Redis and SQLite, to services like AI Search, to allow a user to implement the user's own interface (write their own code). The main difference is that some of these services will have their own search implementation vs databases where the cosine similarity has to be implemented in code.

So the question is what vector database should one choose? This is an important question, and the answer may depend on several factors, including required performance, scalability, the quality of expected results, and cost. For example, If your application only requires a few documents that could be stored as files or embedded resources, say, a command-line interface application, you could choose everything from RAM storage to SQLite. On the other hand, if you were planning to ingest and search a vast number of documents, you may be better served by solutions like Azure AI Search or Azure Cosmos DB.

I like the Semantic Vector approach of using interfaces to configure the vector database. This allows the developer to have an environment in development possibly with a RAM or SQLite store, and then move to Redis or Azure AI Search, in production with a simple switch and without having to change the rest of the code. Here's a C# Polyglot notebook of a simple RAG implementation using this approach: https://lnkd.in/e-aw44w7

This topic is complex because there also needs to be a discussion around the ingestion process including reading the full text from documents, chunking the text for embedding, storing the text and the embedding, querying, and the logic for using the ranked results. Then there's the topic of models. ADA-002, for example, takes up to 8191 tokens and produces a 1535-dimensional embedding vector. About querying, dedicated services like Azure AI Search implement their own search functionality that can improve the quality of the ranked results.

I hope this post helps you make better decisions when selecting the next vector database in your project.
